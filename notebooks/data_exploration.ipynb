{
	"cells": [
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"# Construction Crack Detection - Data Exploration\n",
				"\n",
				"This notebook explores the dataset for crack detection in construction images."
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"import os\n",
				"import sys\n",
				"import numpy as np\n",
				"import matplotlib.pyplot as plt\n",
				"import cv2\n",
				"from pathlib import Path\n",
				"import random\n",
				"from tqdm import tqdm\n",
				"\n",
				"# Add project root to path\n",
				"sys.path.append('..')\n",
				"from crackdetect.data.preprocessing import ImagePreprocessor\n",
				"from crackdetect.utils.crack_analysis import CrackAnalyzer"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"## 1. Dataset Overview\n",
				"\n",
				"Let's first define the paths to our dataset."
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"# Set paths\n",
				"data_dir = Path(\"../data\")\n",
				"train_dir = data_dir / \"train\"\n",
				"val_dir = data_dir / \"val\"\n",
				"test_dir = data_dir / \"test\"\n",
				"\n",
				"train_images_dir = train_dir / \"images\"\n",
				"train_masks_dir = train_dir / \"masks\"\n",
				"\n",
				"# Check if directories exist\n",
				"print(f\"Train images directory exists: {train_images_dir.exists()}\")\n",
				"print(f\"Train masks directory exists: {train_masks_dir.exists()}\")\n",
				"print(f\"Validation directory exists: {val_dir.exists()}\")\n",
				"print(f\"Test directory exists: {test_dir.exists()}\")"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"### Data Statistics"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"# Count images in each directory\n",
				"train_images = list(train_images_dir.glob(\"*.jpg\")) + list(train_images_dir.glob(\"*.png\"))\n",
				"train_masks = list(train_masks_dir.glob(\"*.png\"))\n",
				"\n",
				"print(f\"Number of training images: {len(train_images)}\")\n",
				"print(f\"Number of training masks: {len(train_masks)}\")\n",
				"\n",
				"if val_dir.exists():\n",
				"    val_images = list((val_dir / \"images\").glob(\"*.jpg\")) + list((val_dir / \"images\").glob(\"*.png\"))\n",
				"    val_masks = list((val_dir / \"masks\").glob(\"*.png\"))\n",
				"    print(f\"Number of validation images: {len(val_images)}\")\n",
				"    print(f\"Number of validation masks: {len(val_masks)}\")\n",
				"\n",
				"if test_dir.exists():\n",
				"    test_images = list((test_dir / \"images\").glob(\"*.jpg\")) + list((test_dir / \"images\").glob(\"*.png\"))\n",
				"    print(f\"Number of test images: {len(test_images)}\")"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"## 2. Image Visualization\n",
				"\n",
				"Let's visualize some sample images and their masks to understand the dataset."
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"# Initialize preprocessor\n",
				"preprocessor = ImagePreprocessor()\n",
				"\n",
				"# Function to display images with masks\n",
				"def display_images_with_masks(image_paths, mask_paths, num_samples=5):\n",
				"    # Randomly sample images\n",
				"    indices = random.sample(range(len(image_paths)), min(num_samples, len(image_paths)))\n",
				"    \n",
				"    fig, axes = plt.subplots(num_samples, 3, figsize=(15, 5*num_samples))\n",
				"    fig.tight_layout(pad=3.0)\n",
				"    \n",
				"    for i, idx in enumerate(indices):\n",
				"        img_path = image_paths[idx]\n",
				"        mask_path = mask_paths[idx] if idx < len(mask_paths) else None\n",
				"        \n",
				"        # Read image\n",
				"        image = preprocessor.read_image(img_path)\n",
				"        \n",
				"        # Read mask if available\n",
				"        if mask_path:\n",
				"            mask = cv2.imread(str(mask_path), cv2.IMREAD_GRAYSCALE)\n",
				"            mask = (mask > 0).astype(np.uint8) * 255\n",
				"            \n",
				"            # Create overlay\n",
				"            overlay = image.copy()\n",
				"            overlay[mask > 0] = [255, 0, 0]\n",
				"            overlay = cv2.addWeighted(image, 0.7, overlay, 0.3, 0)\n",
				"        else:\n",
				"            mask = np.zeros((image.shape[0], image.shape[1]), dtype=np.uint8)\n",
				"            overlay = image.copy()\n",
				"        \n",
				"        # Display images\n",
				"        axes[i, 0].imshow(image)\n",
				"        axes[i, 0].set_title(f\"Image: {img_path.name}\")\n",
				"        axes[i, 0].axis('off')\n",
				"        \n",
				"        axes[i, 1].imshow(mask, cmap='gray')\n",
				"        axes[i, 1].set_title(\"Mask\")\n",
				"        axes[i, 1].axis('off')\n",
				"        \n",
				"        axes[i, 2].imshow(overlay)\n",
				"        axes[i, 2].set_title(\"Overlay\")\n",
				"        axes[i, 2].axis('off')\n",
				"    \n",
				"    plt.show()\n",
				"\n",
				"# Display training images\n",
				"display_images_with_masks(train_images, train_masks, num_samples=5)"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"## 3. Image Preprocessing\n",
				"\n",
				"Let's explore the preprocessing steps and see how they affect the images."
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"# Function to display preprocessing steps\n",
				"def display_preprocessing_steps(image_path):\n",
				"    # Read image\n",
				"    original = preprocessor.read_image(image_path)\n",
				"    \n",
				"    # Apply preprocessing steps\n",
				"    denoised = preprocessor.denoise(original)\n",
				"    enhanced = preprocessor.enhance_contrast(denoised)\n",
				"    resized = preprocessor.resize(enhanced)\n",
				"    normalized = preprocessor.normalize(resized)\n",
				"    edges = preprocessor.extract_edges(resized)\n",
				"    \n",
				"    # Display results\n",
				"    fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
				"    fig.tight_layout(pad=3.0)\n",
				"    \n",
				"    axes[0, 0].imshow(original)\n",
				"    axes[0, 0].set_title(\"Original Image\")\n",
				"    axes[0, 0].axis('off')\n",
				"    \n",
				"    axes[0, 1].imshow(denoised)\n",
				"    axes[0, 1].set_title(\"Denoised\")\n",
				"    axes[0, 1].axis('off')\n",
				"    \n",
				"    axes[0, 2].imshow(enhanced)\n",
				"    axes[0, 2].set_title(\"Contrast Enhanced\")\n",
				"    axes[0, 2].axis('off')\n",
				"    \n",
				"    axes[1, 0].imshow(resized)\n",
				"    axes[1, 0].set_title(f\"Resized to {resized.shape[:2]}\")\n",
				"    axes[1, 0].axis('off')\n",
				"    \n",
				"    axes[1, 1].imshow(normalized)\n",
				"    axes[1, 1].set_title(\"Normalized [0, 1]\")\n",
				"    axes[1, 1].axis('off')\n",
				"    \n",
				"    axes[1, 2].imshow(edges, cmap='gray')\n",
				"    axes[1, 2].set_title(\"Edge Detection\")\n",
				"    axes[1, 2].axis('off')\n",
				"    \n",
				"    plt.show()\n",
				"\n",
				"# Display preprocessing steps for a sample image\n",
				"sample_image = random.choice(train_images)\n",
				"display_preprocessing_steps(sample_image)"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"## 4. Crack Analysis\n",
				"\n",
				"Let's analyze the properties of cracks in our dataset."
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"# Initialize crack analyzer\n",
				"analyzer = CrackAnalyzer(pixel_mm_ratio=1.0)\n",
				"\n",
				"# Function to analyze cracks in an image\n",
				"def analyze_sample_cracks(image_path, mask_path, pixel_mm_ratio=1.0):\n",
				"    # Read image and mask\n",
				"    image = preprocessor.read_image(image_path)\n",
				"    mask = cv2.imread(str(mask_path), cv2.IMREAD_GRAYSCALE)\n",
				"    mask = (mask > 0).astype(np.float32)\n",
				"    \n",
				"    # Preprocess image\n",
				"    processed_image = preprocessor.preprocess(image)\n",
				"    \n",
				"    # Set pixel to mm ratio\n",
				"    analyzer.pixel_mm_ratio = pixel_mm_ratio\n",
				"    \n",
				"    # Analyze cracks\n",
				"    crack_properties = analyzer.analyze_mask(mask)\n",
				"    \n",
				"    # Visualize results\n",
				"    result_image = analyzer.visualize_analysis(processed_image, mask, crack_properties)\n",
				"    \n",
				"    # Display results\n",
				"    fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
				"    \n",
				"    axes[0].imshow(image)\n",
				"    axes[0].set_title(\"Original Image\")\n",
				"    axes[0].axis('off')\n",
				"    \n",
				"    axes[1].imshow(mask, cmap='gray')\n",
				"    axes[1].set_title(\"Crack Mask\")\n",
				"    axes[1].axis('off')\n",
				"    \n",
				"    axes[2].imshow(result_image)\n",
				"    axes[2].set_title(\"Crack Analysis\")\n",
				"    axes[2].axis('off')\n",
				"    \n",
				"    plt.tight_layout()\n",
				"    plt.show()\n",
				"    \n",
				"    # Print crack properties\n",
				"    print(f\"Number of cracks detected: {len(crack_properties)}\")\n",
				"    for i, props in enumerate(crack_properties):\n",
				"        print(f\"\\nCrack #{i+1}:\")\n",
				"        print(f\"  Severity: {props.severity}\")\n",
				"        print(f\"  Average Width: {props.width_avg:.2f} mm\")\n",
				"        print(f\"  Maximum Width: {props.width_max:.2f} mm\")\n",
				"        print(f\"  Length: {props.length:.2f} mm\")\n",
				"        print(f\"  Area: {props.area:.2f} mm²\")\n",
				"        print(f\"  Orientation: {props.orientation:.1f}°\")\n",
				"\n",
				"# Select a sample image with mask\n",
				"sample_idx = random.randint(0, len(train_images) - 1)\n",
				"sample_image = train_images[sample_idx]\n",
				"sample_mask = train_masks[sample_idx]\n",
				"\n",
				"# Analyze sample cracks\n",
				"analyze_sample_cracks(sample_image, sample_mask, pixel_mm_ratio=0.1)"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"## 5. Dataset Statistics\n",
				"\n",
				"Let's calculate some statistics about the dataset."
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"# Function to calculate dataset statistics\n",
				"def calculate_dataset_statistics(image_paths, mask_paths):\n",
				"    # Statistics to collect\n",
				"    image_sizes = []\n",
				"    mask_ratios = []\n",
				"    crack_counts = []\n",
				"    crack_widths = []\n",
				"    crack_lengths = []\n",
				"    crack_areas = []\n",
				"    \n",
				"    # Process each image\n",
				"    for i, (img_path, mask_path) in enumerate(tqdm(zip(image_paths, mask_paths), total=len(image_paths))):\n",
				"        # Limit the number of images to process for efficiency\n",
				"        if i >= 50:  # Process only 50 images for this analysis\n",
				"            break\n",
				"        \n",
				"        # Read image and mask\n",
				"        image = preprocessor.read_image(img_path)\n",
				"        mask = cv2.imread(str(mask_path), cv2.IMREAD_GRAYSCALE)\n",
				"        mask = (mask > 0).astype(np.float32)\n",
				"        \n",
				"        # Image size\n",
				"        image_sizes.append(image.shape[:2])\n",
				"        \n",
				"        # Mask ratio (% of pixels that are cracks)\n",
				"        mask_ratio = np.mean(mask) * 100\n",
				"        mask_ratios.append(mask_ratio)\n",
				"        \n",
				"        # Analyze cracks\n",
				"        crack_properties = analyzer.analyze_mask(mask)\n",
				"        \n",
				"        # Crack count\n",
				"        crack_counts.append(len(crack_properties))\n",
				"        \n",
				"        # Crack properties\n",
				"        for props in crack_properties:\n",
				"            crack_widths.append(props.width_avg)\n",
				"            crack_lengths.append(props.length)\n",
				"            crack_areas.append(props.area)\n",
				"    \n",
				"    return {\n",
				"        'image_sizes': image_sizes,\n",
				"        'mask_ratios': mask_ratios,\n",
				"        'crack_counts': crack_counts,\n",
				"        'crack_widths': crack_widths,\n",
				"        'crack_lengths': crack_lengths,\n",
				"        'crack_areas': crack_areas\n",
				"    }\n",
				"\n",
				"# Calculate statistics\n",
				"stats = calculate_dataset_statistics(train_images, train_masks)"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"# Visualize statistics\n",
				"fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
				"fig.tight_layout(pad=3.0)\n",
				"\n",
				"# Image sizes\n",
				"heights = [size[0] for size in stats['image_sizes']]\n",
				"widths = [size[1] for size in stats['image_sizes']]\n",
				"axes[0, 0].scatter(widths, heights)\n",
				"axes[0, 0].set_title('Image Sizes')\n",
				"axes[0, 0].set_xlabel('Width (pixels)')\n",
				"axes[0, 0].set_ylabel('Height (pixels)')\n",
				"axes[0, 0].grid(True)\n",
				"\n",
				"# Mask ratios\n",
				"axes[0, 1].hist(stats['mask_ratios'], bins=20)\n",
				"axes[0, 1].set_title('Crack Coverage Distribution')\n",
				"axes[0, 1].set_xlabel('Crack Coverage (%)')\n",
				"axes[0, 1].set_ylabel('Number of Images')\n",
				"axes[0, 1].grid(True)\n",
				"\n",
				"# Crack counts\n",
				"axes[0, 2].hist(stats['crack_counts'], bins=max(10, max(stats['crack_counts'])))\n",
				"axes[0, 2].set_title('Crack Count Distribution')\n",
				"axes[0, 2].set_xlabel('Number of Cracks per Image')\n",
				"axes[0, 2].set_ylabel('Number of Images')\n",
				"axes[0, 2].grid(True)\n",
				"\n",
				"# Crack widths\n",
				"axes[1, 0].hist(stats['crack_widths'], bins=20)\n",
				"axes[1, 0].set_title('Crack Width Distribution')\n",
				"axes[1, 0].set_xlabel('Crack Width (mm)')\n",
				"axes[1, 0].set_ylabel('Frequency')\n",
				"axes[1, 0].grid(True)\n",
				"\n",
				"# Crack lengths\n",
				"axes[1, 1].hist(stats['crack_lengths'], bins=20)\n",
				"axes[1, 1].set_title('Crack Length Distribution')\n",
				"axes[1, 1].set_xlabel('Crack Length (mm)')\n",
				"axes[1, 1].set_ylabel('Frequency')\n",
				"axes[1, 1].grid(True)\n",
				"\n",
				"# Crack areas\n",
				"axes[1, 2].hist(stats['crack_areas'], bins=20)\n",
				"axes[1, 2].set_title('Crack Area Distribution')\n",
				"axes[1, 2].set_xlabel('Crack Area (mm²)')\n",
				"axes[1, 2].set_ylabel('Frequency')\n",
				"axes[1, 2].grid(True)\n",
				"\n",
				"plt.show()\n",
				"\n",
				"# Print summary statistics\n",
				"print(\"Dataset Statistics:\")\n",
				"print(f\"Number of images analyzed: {len(stats['image_sizes'])}\")\n",
				"print(f\"Average image dimensions: {np.mean(heights):.1f} x {np.mean(widths):.1f} pixels\")\n",
				"print(f\"Average crack coverage: {np.mean(stats['mask_ratios']):.2f}%\")\n",
				"print(f\"Average number of cracks per image: {np.mean(stats['crack_counts']):.2f}\")\n",
				"\n",
				"if stats['crack_widths']:\n",
				"    print(f\"Average crack width: {np.mean(stats['crack_widths']):.2f} mm\")\n",
				"    print(f\"Average crack length: {np.mean(stats['crack_lengths']):.2f} mm\")\n",
				"    print(f\"Average crack area: {np.mean(stats['crack_areas']):.2f} mm²\")"
			]
		}
	],
	"metadata": {
		"kernelspec": {
			"display_name": "Python 3",
			"language": "python",
			"name": "python3"
		},
		"language_info": {
			"codemirror_mode": {
				"name": "ipython",
				"version": 3
			},
			"file_extension": ".py",
			"mimetype": "text/x-python",
			"name": "python",
			"nbconvert_exporter": "python",
			"pygments_lexer": "ipython3",
			"version": "3.8.10"
		}
	},
	"nbformat": 4,
	"nbformat_minor": 4
}
