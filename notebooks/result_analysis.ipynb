{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Construction Crack Detection - Result Analysis\n",
    "\n",
    "This notebook analyzes the results of the crack detection model on test images and provides insights into the model's performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "from tqdm.notebook import tqdm\n",
    "import cv2\n",
    "\n",
    "# Add project root to path\n",
    "sys.path.append('..')\n",
    "from crackdetect.models.segmentation import UNet\n",
    "from crackdetect.data.dataset import CrackDataset\n",
    "from crackdetect.utils.crack_analysis import CrackAnalyzer\n",
    "from crackdetect.inference.predictor import Predictor\n",
    "from crackdetect.inference.visualization import create_result_figure\n",
    "from crackdetect.training.metrics import iou_score, dice_coefficient, precision, recall\n",
    "from config.config import Config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load the Trained Model and Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load configuration\n",
    "config = Config()\n",
    "\n",
    "# Set paths\n",
    "model_path = Path(\"../saved_models/crack_detection_final.pth\")\n",
    "test_dir = Path(\"../data/test\")\n",
    "results_dir = Path(\"../results\")\n",
    "results_dir.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "# Check if model exists\n",
    "if not model_path.exists():\n",
    "    print(f\"Model not found at {model_path}. Please train a model first.\")\n",
    "    model_path = list(Path(\"../saved_models\").glob(\"*.pth\"))[0] if list(Path(\"../saved_models\").glob(\"*.pth\")) else None\n",
    "    if model_path:\n",
    "        print(f\"Using alternative model: {model_path}\")\n",
    "    else:\n",
    "        print(\"No model found. Please train a model first.\")\n",
    "\n",
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Create predictor\n",
    "predictor = Predictor(\n",
    "    model_path=model_path,\n",
    "    device=device,\n",
    "    confidence_threshold=0.5,\n",
    "    pixel_mm_ratio=0.1,  # Adjust as needed\n",
    "    min_crack_area=100\n",
    ")\n",
    "\n",
    "# Load test dataset\n",
    "test_dataset = CrackDataset(\n",
    "    image_dir=test_dir / \"images\",\n",
    "    mask_dir=test_dir / \"masks\" if (test_dir / \"masks\").exists() else None,\n",
    "    image_size=config.image_size,\n",
    "    transform=None,\n",
    "    preprocessing=True\n",
    ")\n",
    "\n",
    "print(f\"Test dataset size: {len(test_dataset)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Analyze Sample Test Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose a sample image\n",
    "sample_idx = 0\n",
    "sample_item = test_dataset[sample_idx]\n",
    "sample_image = sample_item['image'].permute(1, 2, 0).numpy()\n",
    "sample_filename = sample_item['filename']\n",
    "\n",
    "# Get ground truth mask if available\n",
    "has_ground_truth = 'mask' in sample_item\n",
    "if has_ground_truth:\n",
    "    sample_mask = sample_item['mask'][0].numpy()\n",
    "\n",
    "# Analyze image\n",
    "results = predictor.analyze_image(sample_image)\n",
    "\n",
    "# Display results\n",
    "plt.figure(figsize=(18, 12))\n",
    "\n",
    "plt.subplot(2, 2, 1)\n",
    "plt.title(\"Original Image\")\n",
    "plt.imshow(results['processed_image'])\n",
    "plt.axis('off')\n",
    "\n",
    "plt.subplot(2, 2, 2)\n",
    "plt.title(\"Predicted Mask\")\n",
    "plt.imshow(results['prediction_mask'], cmap='gray')\n",
    "plt.axis('off')\n",
    "\n",
    "if has_ground_truth:\n",
    "    plt.subplot(2, 2, 3)\n",
    "    plt.title(\"Ground Truth Mask\")\n",
    "    plt.imshow(sample_mask, cmap='gray')\n",
    "    plt.axis('off')\n",
    "\n",
    "plt.subplot(2, 2, 4)\n",
    "plt.title(\"Crack Analysis Result\")\n",
    "plt.imshow(results['result_image'])\n",
    "plt.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(results_dir / f\"{sample_filename}_analysis.png\", dpi=300)\n",
    "plt.show()\n",
    "\n",
    "# Print analysis results\n",
    "print(f\"Analysis Results for {sample_filename}:\")\n",
    "print(f\"Number of cracks detected: {len(results['crack_properties'])}\")\n",
    "\n",
    "if results['crack_properties']:\n",
    "    for i, props in enumerate(results['crack_properties']):\n",
    "        print(f\"\\nCrack #{i+1}:\")\n",
    "        print(f\"  Severity: {props.severity}\")\n",
    "        print(f\"  Average Width: {props.width_avg:.2f} mm\")\n",
    "        print(f\"  Maximum Width: {props.width_max:.2f} mm\")\n",
    "        print(f\"  Length: {props.length:.2f} mm\")\n",
    "        print(f\"  Area: {props.area:.2f} mm²\")\n",
    "        print(f\"  Orientation: {props.orientation:.1f}°\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Evaluate Model Performance\n",
    "\n",
    "If ground truth masks are available, let's evaluate the model's performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(dataset, predictor):\n",
    "    \"\"\"\n",
    "    Evaluate model performance on a dataset.\n",
    "    \n",
    "    Args:\n",
    "        dataset: CrackDataset with ground truth masks\n",
    "        predictor: Predictor instance\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame with evaluation metrics\n",
    "    \"\"\"\n",
    "    # Check if dataset has ground truth masks\n",
    "    if 'mask' not in dataset[0]:\n",
    "        print(\"No ground truth masks available for evaluation.\")\n",
    "        return None\n",
    "    \n",
    "    # Initialize metrics\n",
    "    metrics = {\n",
    "        'filename': [],\n",
    "        'iou': [],\n",
    "        'dice': [],\n",
    "        'precision': [],\n",
    "        'recall': [],\n",
    "        'num_cracks': [],\n",
    "        'avg_width': [],\n",
    "        'max_width': [],\n",
    "        'total_length': []\n",
    "    }\n",
    "    \n",
    "    # Process each image\n",
    "    for i in tqdm(range(len(dataset)), desc=\"Evaluating\"):\n",
    "        # Get data\n",
    "        item = dataset[i]\n",
    "        image = item['image'].permute(1, 2, 0).numpy()\n",
    "        mask = item['mask'][0].numpy()\n",
    "        filename = item['filename']\n",
    "        \n",
    "        # Predict\n",
    "        results = predictor.analyze_image(image)\n",
    "        pred_mask = results['prediction_mask']\n",
    "        \n",
    "        # Calculate metrics\n",
    "        pred_tensor = torch.tensor(pred_mask).unsqueeze(0).unsqueeze(0)\n",
    "        mask_tensor = torch.tensor(mask).unsqueeze(0).unsqueeze(0)\n",
    "        \n",
    "        iou = iou_score(pred_tensor, mask_tensor).item()\n",
    "        dice = dice_coefficient(pred_tensor, mask_tensor).item()\n",
    "        prec = precision(pred_tensor, mask_tensor).item()\n",
    "        rec = recall(pred_tensor, mask_tensor).item()\n",
    "        \n",
    "        # Record metrics\n",
    "        metrics['filename'].append(filename)\n",
    "        metrics['iou'].append(iou)\n",
    "        metrics['dice'].append(dice)\n",
    "        metrics['precision'].append(prec)\n",
    "        metrics['recall'].append(rec)\n",
    "        \n",
    "        # Record crack properties\n",
    "        crack_properties = results['crack_properties']\n",
    "        metrics['num_cracks'].append(len(crack_properties))\n",
    "        \n",
    "        if crack_properties:\n",
    "            widths = [props.width_avg for props in crack_properties]\n",
    "            max_widths = [props.width_max for props in crack_properties]\n",
    "            lengths = [props.length for props in crack_properties]\n",
    "            \n",
    "            metrics['avg_width'].append(np.mean(widths))\n",
    "            metrics['max_width'].append(np.max(max_widths))\n",
    "            metrics['total_length'].append(np.sum(lengths))\n",
    "        else:\n",
    "            metrics['avg_width'].append(0)\n",
    "            metrics['max_width'].append(0)\n",
    "            metrics['total_length'].append(0)\n",
    "    \n",
    "    # Create DataFrame\n",
    "    df = pd.DataFrame(metrics)\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Only evaluate if ground truth masks are available\n",
    "if 'mask' in test_dataset[0]:\n",
    "    eval_df = evaluate_model(test_dataset, predictor)\n",
    "    \n",
    "    # Save results\n",
    "    eval_df.to_csv(results_dir / \"evaluation_metrics.csv\", index=False)\n",
    "    \n",
    "    # Print summary\n",
    "    print(\"\\nEvaluation Summary:\")\n",
    "    print(f\"Average IoU: {eval_df['iou'].mean():.4f}\")\n",
    "    print(f\"Average Dice: {eval_df['dice'].mean():.4f}\")\n",
    "    print(f\"Average Precision: {eval_df['precision'].mean():.4f}\")\n",
    "    print(f\"Average Recall: {eval_df['recall'].mean():.4f}\")\n",
    "    print(f\"Average Cracks per Image: {eval_df['num_cracks'].mean():.2f}\")\n",
    "else:\n",
    "    print(\"No ground truth masks available for evaluation.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Visualize Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only visualize if evaluation was performed\n",
    "if 'eval_df' in locals():\n",
    "    # Set up the figure\n",
    "    plt.figure(figsize=(15, 10))\n",
    "    \n",
    "    # IoU distribution\n",
    "    plt.subplot(2, 2, 1)\n",
    "    sns.histplot(eval_df['iou'], kde=True)\n",
    "    plt.title('IoU Distribution')\n",
    "    plt.xlabel('IoU')\n",
    "    plt.ylabel('Count')\n",
    "    plt.axvline(eval_df['iou'].mean(), color='r', linestyle='--', label=f'Mean: {eval_df[\"iou\"].mean():.3f}')\n",
    "    plt.legend()\n",
    "    \n",
    "    # Dice distribution\n",
    "    plt.subplot(2, 2, 2)\n",
    "    sns.histplot(eval_df['dice'], kde=True)\n",
    "    plt.title('Dice Coefficient Distribution')\n",
    "    plt.xlabel('Dice')\n",
    "    plt.ylabel('Count')\n",
    "    plt.axvline(eval_df['dice'].mean(), color='r', linestyle='--', label=f'Mean: {eval_df[\"dice\"].mean():.3f}')\n",
    "    plt.legend()\n",
    "    \n",
    "    # Precision vs. Recall scatter plot\n",
    "    plt.subplot(2, 2, 3)\n",
    "    plt.scatter(eval_df['precision'], eval_df['recall'], alpha=0.6)\n",
    "    plt.title('Precision vs. Recall')\n",
    "    plt.xlabel('Precision')\n",
    "    plt.ylabel('Recall')\n",
    "    plt.grid(True)\n",
    "    plt.xlim(0, 1)\n",
    "    plt.ylim(0, 1)\n",
    "    \n",
    "    # Number of cracks distribution\n",
    "    plt.subplot(2, 2, 4)\n",
    "    sns.countplot(x='num_cracks', data=eval_df)\n",
    "    plt.title('Number of Cracks per Image')\n",
    "    plt.xlabel('Number of Cracks')\n",
    "    plt.ylabel('Count')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(results_dir / \"metrics_distribution.png\", dpi=300)\n",
    "    plt.show()\n",
    "    \n",
    "    # Correlation between metrics\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    correlation_metrics = ['iou', 'dice', 'precision', 'recall', 'num_cracks', 'avg_width', 'max_width', 'total_length']\n",
    "    corr = eval_df[correlation_metrics].corr()\n",
    "    \n",
    "    sns.heatmap(corr, annot=True, cmap='coolwarm', vmin=-1, vmax=1, center=0)\n",
    "    plt.title('Correlation Between Metrics')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(results_dir / \"metrics_correlation.png\", dpi=300)\n",
    "    plt.show()\n",
    "    \n",
    "    # Boxplot of IoU for different number of cracks\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    sns.boxplot(x='num_cracks', y='iou', data=eval_df)\n",
    "    plt.title('IoU vs. Number of Cracks')\n",
    "    plt.xlabel('Number of Cracks')\n",
    "    plt.ylabel('IoU')\n",
    "    plt.grid(True)\n",
    "    plt.savefig(results_dir / \"iou_vs_cracks.png\", dpi=300)\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}